servicesDomain: ""

ingressClass: nginx

authProxy:
  name: auth-proxy
  image: quay.io/mojanalytics/auth-proxy
  tag: v5.2.6
  imagePullPolicy: IfNotPresent
  containerPort: 3000
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 25m
      memory: 64Mi
  cookie_maxage: 36000 #10 hours
  cookie_secret: "tony-montana"

apps:
  - name: alertmanager
    port: 9093
  - name: prometheus
    port: 9090

serviceMonitors:
  - labelSelector: nginx-ingress
    targetNamespace: default
    targetPort: metrics
 # - labelSelector: kubernetes-dashboard
 #   targetNamespace: kube-system
 #   targetPath: /metrics
 #   targetPort: 8443
 #   scrapeInterval: 20s

image:
  pullSecrets: []

prometheusOperator:
  crdApiGroup: monitoring.coreos.com
  chartName: prometheus-operator

prometheusRules:
    - name: analytics.rules
      rules:
      - alert: InstanceHighMemoryUsage
        annotations:
          message: '{{ $value }}% of cluster memory is in use'
          summary: High Memory Load
         # Alert if disk utilisation reaches 90%
        expr: (sum(node_memory_MemTotal_bytes) - sum(node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes) ) / sum(node_memory_MemTotal_bytes) * 100 > 90
        for: 10m
        labels:
          severity: warning

      - alert: PredictiveHostDiskSpace
        annotations:
          message: 'Based on recent sampling, the disk is likely to will fill on volume
             {{ $labels.mountpoint }} within the next 5 hours for instace: {{ $labels.instance_id
             }} tagged as: {{ $labels.instance_name_tag }}'
          summary: Predictive Disk Space Utilisation Alert
        expr: predict_linear(node_filesystem_free_bytes{mountpoint="/"}[4h], 5 * 3600) < 0
        for: 10h
        labels:
          severity: warning

      - alert: HighNumberOfUserHTTPErrors
        annotations:
          message: 'Ingress: {{ $labels.ingress }} has returned {{ $value }} 503 errors in the last 10 minutes'
          summary: User getting a high number of 503 errors
        expr: increase(nginx_ingress_controller_requests{status=~"503", exported_namespace=~"user-.*"}[10m]) > 5
        for: 1m
        label:
          severity: warning

      - alert: KubeDNSErrors
        annotations:
          message: 'KubeDNS: {{ $labels.pod }} has returned {{ $value }} errors in the last 10 minutes'
          summary: KubeDNS pod errors
        expr: increase(kubedns_dnsmasq_errors[10m]) > 0
        for: 1m
        label:
          severity: critical

# custom kubernetes rules
    - name: kubernetes-system
      rules:
      - alert: KubeNodeNotReady
        annotations:
          message: '{{ $labels.node }} has been unready for more than 5 minutes.'
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready
        expr: kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
      - alert: KubeVersionMismatch
        annotations:
          message: There are {{ $value }} different semantic versions of Kubernetes components running.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch
        expr: count(count by (gitVersion) (label_replace(kubernetes_build_info{job!="kube-dns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
        for: 1h
        labels:
          severity: warning
      - alert: KubeClientErrors
        annotations:
          message: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ printf "%0.0f" $value }}% errors.'
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
        expr: |-
          (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job)
            /
          sum(rate(rest_client_requests_total[5m])) by (instance, job))
          * 100 > 1
        for: 15m
        labels:
          severity: warning
      - alert: KubeClientErrors
        annotations:
          message: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ printf "%0.0f" $value }} errors / second.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
        expr: sum(rate(ksm_scrape_error_total{job="kube-state-metrics"}[5m])) by (instance, job) > 0.1
        for: 15m
        labels:
          severity: warning
      - alert: KubeletTooManyPods
        annotations:
          message: Kubelet {{ $labels.instance }} is running {{ $value }} Pods, close to the limit of 110.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods
        expr: kubelet_running_pod_count{job="kubelet"} > 110 * 0.9
        for: 15m
        labels:
          severity: warning
      - alert: KubeAPILatencyHigh
        annotations:
          message: The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
        expr: cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 1
        for: 10m
        labels:
          severity: warning
      - alert: KubeAPILatencyHigh
        annotations:
          message: The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
        expr: cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 4
        for: 10m
        labels:
          severity: critical
      - alert: KubeAPIErrorsHigh
        annotations:
          message: API server is returning errors for {{ $value }}% of requests.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
        expr: |-
          sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m])) without(instance, pod)
            /
          sum(rate(apiserver_request_count{job="apiserver"}[5m])) without(instance, pod) * 100 > 10
        for: 10m
        labels:
          severity: critical
      - alert: KubeAPIErrorsHigh
        annotations:
          message: API server is returning errors for {{ $value }}% of requests.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
        expr: |-
          sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[5m])) without(instance, pod)
            /
          sum(rate(apiserver_request_count{job="apiserver"}[5m])) without(instance, pod) * 100 > 5
        for: 10m
        labels:
          severity: warning
      - alert: KubeClientCertificateExpiration
        annotations:
          message: A client certificate used to authenticate to the apiserver is expiring in less than 7 days.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
        expr: histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 604800
        labels:
          severity: warning
      - alert: KubeClientCertificateExpiration
        annotations:
          message: A client certificate used to authenticate to the apiserver is expiring in less than 24 hours.
          runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
        expr: histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 86400
        labels:
          severity: critical
